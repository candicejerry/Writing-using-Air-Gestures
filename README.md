# Writing-using-Air-Gestures

## Introduction

This project is a combination of object detection and image classification based on the IOS edge device. Through the device’s camera, detecting the user’s fingertip and tracking the movement of the fingertip with a black line. And then using image classification to recognize the digits that users have written.

> Instructions: one finger for drawing, two fingers for pausing drawing, three fingers for clearing the screen.

# ![Image description](https://docs.google.com/uc?export=download&id=1tG3LsgnMbuZcsekFZQT9Ya6_RaFptNnz)

## Video demo
[Demo](https://drive.google.com/open?id=1USy7P8v0_BMWt6moBg2X78Au7JBwgGZI)

## Report
The [report](https://drive.google.com/open?id=1jyEodGnFKugR1xqLvoQct8RwHDqmLDMX) contains all the details about the project.

## Colab Notebook
[Google Colab](https://drive.google.com/open?id=1UzoT5bEjTK-3xBk4jSxnT-T3FgIiluXT)

## References
CPPN: http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/ <br />
Google edge TF Lite IOS tutorial: https://cloud.google.com/vision/automl/object-detection/docs/tflite-ios-tutorial <br />
Kaggle dataset "Fingers": https://www.kaggle.com/koryakinp/fingers (Unused for the final version)
